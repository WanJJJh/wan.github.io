---
title: 'DPO'
date: 2024-10-12
permalink: /research/2024-10-12/
tags:
  - RLHF
  - LLM
---

大模型人类偏好数据训练算法，DPO。

# DPO

奖励模型训练损失：（win，lose）

$$
L_R(r_{\phi},D)=-E_{(x,y_w,y_l)\sim D}[\log\sigma(r_\phi(x,y_w)-r_\phi(x,y_l))]
$$

RL训练目标：

$$
\underset{\pi_\theta}{\max}E_{x\sim D,y\sim \pi_\theta(y|x)}[r_\phi(x,y)]-\beta D_{KL}[\pi_\theta(y|x)||\pi_{ref}(y|x)]
$$

---

首先，我们假定已经有了一个完备的reward，其可以准确的计算$r(x,y)$

推导RL的训练目标：

$$
\begin{align*}
&\underset{\pi}{\max}\mathbb{E}_{x\sim D,y\sim \pi(y|x)}[r(x,y)]-\beta D_{KL}[\pi(y|x)||\pi_{ref}(y|x)]\\
=&\underset{\pi}{\max}\mathbb{E}_{x\sim D}\mathbb{E}_{y\sim \pi(y|x)}[r(x,y)-\beta\log\frac{\pi(y|x)}{\pi_{ref}(y|x)}]\\
=&\underset{\pi}{\min}\mathbb{E}_{x\sim D}\mathbb{E}_{y\sim \pi(y|x)}[\log\frac{\pi(y|x)}{\pi_{ref}(y|x)}-\frac{1}{\beta}r(x,y)]\\
=&\underset{\pi}{\min}\mathbb{E}_{x\sim D}\mathbb{E}_{y\sim \pi(y|x)}[\log\frac{\pi(y|x)}{\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))}]\\
=&\underset{\pi}{\min}\mathbb{E}_{x\sim D}\mathbb{E}_{y\sim \pi(y|x)}[\log\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))}-\log Z(x)]\\
\end{align*}
$$

其中，这个设定是精华：

$$
Z(x)=\underset{y}\sum\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))
$$

那么，分母也可以表示为一个模型：

$$
\pi^*(y|x)=\frac{1}{Z(x)}\pi_{ref}(y|x)exp(\frac{1}{\beta}r(x,y))
$$

由于 $Z(x)$ 与 $\pi$ 无关，且KL散度最小，即为两个分布一致：

$$
\begin{align*}
=&\underset{\pi}{\min}\mathbb{E}_{x\sim D}\mathbb{E}_{y\sim \pi(y|x)}[\log\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{ref}(y|x)\exp(\frac{1}{\beta}r(x,y))}-\log Z(x)]\\
=&\underset{\pi}{\min}\mathbb{E}_{x\sim D}[D_{KL}(\pi(y|x)||\pi^*(y|x))]
\end{align*}
$$

得到：

$$
\pi(y|x)=\pi^*(y|x)=\frac{1}{Z(x)}\pi_{ref}(y\|x)\exp(\frac{1}{\beta}r(x,y))
$$

---

最优模型是r(x,y)的一个表达式，那么在训练完r(x,y)后，我们遍可以直接得到最优模型

但是这个表达式中Z(x)是比较计算消耗的(需要所有可能y的值)，想办法消除Z(x)

先得到r(x,y)对于最优模型的表达式，这意味着不需要奖励模型，而是重参数化就可以得到：

$$
r(x,y)=\beta\log\frac{\pi_r(y|x)}{\pi_{ref}(y|x)}+\beta\log Z(x)
$$

那么，奖励模型训练的损失函数相减正好可以抵消Z，得到最终的唯一损失：

$$
L_{DPO}(\pi_{\theta};\pi_{ref})=-E_{(x,y_w,y_l)\sim D}[\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}-\beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}]
$$

奖励模型和最优模型之间是重参数化的关系

奖励模型训练好了，最优模型自然也训练好了

！！！

损失的梯度为如下：

$$
\begin{align*}
&\nabla_\theta L_{DPO}(\pi_{\theta};\pi_{ref})=\\
&-\beta E_{(x,y_w,y_l)\sim D}[\sigma(\hat{r}_\theta(x,y_l)-\hat{r}_\theta(x,y_w))[\nabla_\theta\log\pi_\theta(y_w|x)-\nabla_\theta\log\pi_\theta(y_l|x)]]
\end{align*}
$$

其中 $\hat{r}_\theta(x,y)=\beta\log\frac{a}{b}$

其中 $\hat{r}_\theta(x,y)=\beta\log\frac{\pi_a}{\pi_b}$

其中 $\hat{r}_\theta(x,y)=\beta\log\frac{\pi_\theta(a)}{\pi_b}$

其中 $\hat{r}_\theta(x,y)=\beta\log\frac{\pi_\theta(a)}{\pi_{ref}(b)}$

其中 $\hat{r}_\theta(x,y)=\beta\log\frac{\pi_\theta(y\|x)}{\pi_{ref}(y\|x)}$

拆解为两项，先是最大话与win的似然，最小话与lose的似然；二是乘上一个权重，奖励偏差越大，权重越大